{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install networkx nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rafaelmacalaba/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Consolidation using networkx\n",
    "\n",
    "This notebook demonstrates how to consolidate dataset mentions within a given text using `networkx` and other NLP techniques. The process involves identifying and grouping sentences that mention datasets, extracting relevant spans, and structuring the data to the appropriate format.\n",
    "\n",
    "Steps include:\n",
    "1. Tokenizing the text into sentences.\n",
    "2. Using TF-IDF and cosine similarity to find the best matching spans for dataset mentions.\n",
    "3. Building a graph of sentence indices to identify connected components representing grouped mentions.\n",
    "4. Consolidating the dataset mentions into a structured format.\n",
    "\n",
    "The following sections will walk through the implementation and usage of the functions defined for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def find_best_matching_span(text, snippet, window: int = 1):\n",
    "    sents = sent_tokenize(text)\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1, 3))\n",
    "    mi_vec = tfidf.fit_transform([snippet])\n",
    "    sents_vec = tfidf.transform(sents)\n",
    "\n",
    "    mx_idx = cosine_similarity(mi_vec, sents_vec).flatten().argmax()\n",
    "    span_sents = sents[max(mx_idx - window, 0) : min(mx_idx + window + 1, len(sents))]\n",
    "\n",
    "    return {\n",
    "        \"match_idx\": mx_idx,\n",
    "        \"match_sent\": sents[mx_idx],\n",
    "        \"match_span_sents\": span_sents,\n",
    "        \"match_span\": \" \".join(span_sents),\n",
    "    }\n",
    "\n",
    "\n",
    "def find_empirical_span(\n",
    "    text: str, sentences: list, best_match_idx: int, window: int = 1\n",
    "):\n",
    "    # Define the start and end indices to include adjacent sentences for context\n",
    "    start_idx = text.index(sentences[max(best_match_idx - window, 0)])\n",
    "    last_sent = sentences[min(best_match_idx + window, len(sentences) - 1)]\n",
    "    # NOTE: This will fail if the last_sent also occurred in an earlier part of the text.\n",
    "    # SOLUTION: Start the search for last_sent from the start_idx\n",
    "    end_idx = start_idx + text[start_idx:].index(last_sent) + len(last_sent)\n",
    "\n",
    "    # Extract the final span\n",
    "    context_span = text[start_idx:end_idx]\n",
    "\n",
    "    return {\n",
    "        \"empirical_span\": context_span,  # Extracted span\n",
    "        \"start_idx\": start_idx,\n",
    "        \"end_idx\": end_idx,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_empirical_mentioned_in(\n",
    "    text, mentioned_in, window: int = 1, with_match_output: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract the most relevant span of text from the original document (`text`)\n",
    "    that matches the `mentioned_in` field. Returns the span, label, start, and end indices.\n",
    "    \"\"\"\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    match_output = find_best_matching_span(text, mentioned_in, window=window)\n",
    "    best_match_idx = match_output[\"match_idx\"]\n",
    "\n",
    "    output = find_empirical_span(text, sentences, best_match_idx, window=window)\n",
    "    output[\"empirical_mentioned_in\"] = output.pop(\"empirical_span\")\n",
    "\n",
    "    output = {\n",
    "        \"label\": \"mentioned_in\",  # Label as \"mentioned_in\"\n",
    "        **output,\n",
    "    }\n",
    "\n",
    "    if with_match_output:\n",
    "        output.update(match_output)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load helper functions\n",
    "\n",
    "from copy import deepcopy\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def consolidate_dataset(raw_text: str, data: dict):\n",
    "    text = raw_text\n",
    "    page_data = {\"dataset_used\": data.get(\"dataset_used\", False), \"data_mentions\": []}\n",
    "\n",
    "    G = nx.Graph()\n",
    "    sents = sent_tokenize(text)\n",
    "    _datasets = []\n",
    "\n",
    "    for ds in data.get(\"dataset\", []):\n",
    "        mentioned_in = ds.pop(\"mentioned_in\") or \"\"\n",
    "\n",
    "        try:\n",
    "            mi = find_best_matching_span(mentioned_in, ds[\"raw_name\"], window=0)\n",
    "            mi = mi[\"match_span\"]\n",
    "            match_output = find_best_matching_span(text, mi, window=1)\n",
    "        except ValueError:\n",
    "            # Likely that the `mentioned_in` is not found in the text or not correct.\n",
    "            # We try expanding the search to the entire text.\n",
    "            match_output = find_best_matching_span(text, ds[\"raw_name\"], window=1)\n",
    "\n",
    "        ds[\"sent_spans\"] = match_output[\"match_span_sents\"]\n",
    "        sents_idx = sorted([sents.index(s) for s in ds[\"sent_spans\"]])\n",
    "        ds[\"sent\"] = match_output[\"match_sent\"]\n",
    "        ds[\"sent_idx\"] = sents_idx\n",
    "\n",
    "        G.add_edges_from(zip(sents_idx[:-1], sents_idx[1:]))\n",
    "        _datasets.append(ds)\n",
    "\n",
    "    _datasets = sorted(_datasets, key=lambda x: x[\"sent_idx\"][0])\n",
    "\n",
    "    # The connected components in the graphs form the `mentioned_in`s.\n",
    "    mentioned_ins = sorted(\n",
    "        [sorted(x) for x in nx.connected_components(G)], key=lambda x: x[0]\n",
    "    )\n",
    "    updated_mentions = []\n",
    "\n",
    "    for midx in mentioned_ins:\n",
    "        _mi = {\"mentioned_in\": \" \".join([sents[i] for i in midx]), \"datasets\": []}\n",
    "\n",
    "        for ds in _datasets:\n",
    "            ds = deepcopy(ds)\n",
    "            if ds[\"sent_idx\"][0] in midx:\n",
    "                ds.pop(\"sent_idx\")\n",
    "                ds.pop(\"sent_spans\")\n",
    "                _mi[\"datasets\"].append(ds)\n",
    "\n",
    "        updated_mentions.append(_mi)\n",
    "\n",
    "    page_data[\"data_mentions\"] = updated_mentions\n",
    "\n",
    "    return page_data\n",
    "\n",
    "\n",
    "def save_output_per_document(raw_text, data, output_path, page_idx):\n",
    "    \"\"\"\n",
    "    Save output data to a JSON file per document, appending new page data.\n",
    "\n",
    "    Parameters:\n",
    "        data (LabelledResponseFormat): The data to save, in the validated format.\n",
    "        output_path (str): The output path for the document-wide JSON file.\n",
    "        page_idx (int): The current page index being processed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Restructure and consolidate dataset if possible\n",
    "    page_data = consolidate_dataset(raw_text, data)\n",
    "\n",
    "    # Initialize the new page's data structure\n",
    "    page_data = {\"page\": page_idx + 1, **page_data}\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if os.path.exists(output_path):\n",
    "        with open(output_path, \"r\", encoding=\"utf-8\") as existing_file:\n",
    "            document_data = json.load(existing_file)\n",
    "    else:\n",
    "        # Create a new JSON structure\n",
    "        document_data = {\n",
    "            \"source\": os.path.splitext(os.path.basename(output_path))[0],\n",
    "            \"pages\": [],\n",
    "        }\n",
    "\n",
    "    # Append the new page data\n",
    "    document_data[\"pages\"].append(page_data)\n",
    "\n",
    "    # Save the updated document data back to the file\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        json.dump(document_data, output_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example raw text and data\n",
    "raw_text = \"\"\"\n",
    "This is an introductory paragraph that has nothing to do with datasets. It talks about general topics \n",
    "and provides no useful information about machine learning or datasets.\n",
    "\n",
    "The dataset used in this study is the MNIST dataset, which is widely used for image recognition tasks. \n",
    "Another dataset mentioned is the CIFAR-10 dataset, which is commonly used for object recognition. \n",
    "Both datasets are crucial for benchmarking machine learning models.\n",
    "\n",
    "Here is another unrelated paragraph discussing the weather and how sunny days are great for outdoor activities. \n",
    "It has no connection to the datasets or the study being conducted.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"dataset_used\": True,\n",
    "    \"dataset\": [\n",
    "        {\n",
    "            \"raw_name\": \"MNIST\",\n",
    "            \"mentioned_in\": \"The dataset used in this study is the MNIST dataset.\",\n",
    "        },\n",
    "        {\n",
    "            \"raw_name\": \"CIFAR-10\",\n",
    "            \"mentioned_in\": \"Another dataset mentioned is the CIFAR-10 dataset.\",\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Use the consolidate_dataset function\n",
    "consolidated_data = consolidate_dataset(raw_text, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"dataset_used\": true,\n",
      "    \"data_mentions\": [\n",
      "        {\n",
      "            \"mentioned_in\": \"It talks about general topics \\nand provides no useful information about machine learning or datasets. The dataset used in this study is the MNIST dataset, which is widely used for image recognition tasks. Another dataset mentioned is the CIFAR-10 dataset, which is commonly used for object recognition. Both datasets are crucial for benchmarking machine learning models.\",\n",
      "            \"datasets\": [\n",
      "                {\n",
      "                    \"raw_name\": \"MNIST\",\n",
      "                    \"sent\": \"The dataset used in this study is the MNIST dataset, which is widely used for image recognition tasks.\"\n",
      "                },\n",
      "                {\n",
      "                    \"raw_name\": \"CIFAR-10\",\n",
      "                    \"sent\": \"Another dataset mentioned is the CIFAR-10 dataset, which is commonly used for object recognition.\"\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Print the consolidated data\n",
    "print(json.dumps(consolidated_data, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai4data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
