{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11830793,"sourceType":"datasetVersion","datasetId":7432370},{"sourceId":11830766,"sourceType":"datasetVersion","datasetId":7335751}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GLiNER Finetuned Model Output analysis on PRWP documents","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install rapidfuzz","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport glob\n\n#fnames2 = []\n#fnames2 = glob.glob(\"/kaggle/input/onedrive-prwp-outputs/all_prwp_finetuned_output_part1/output/*.json\")\n#fnames1 = glob.glob(\"/kaggle/input/onedrive-prwp-outputs/all_prwp_finetuned_output_part2/output/*.json\")\n\nfnames = glob.glob(\"/kaggle/input/prwp-gliner-outputs/output/*.json\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(fnames)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm.auto import tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"res_df = pd.DataFrame()\nfor fname in tqdm(fnames, desc=\"processing dfs\"):\n    res_read = pd.read_json(fname)\n    if res_read.shape[0] == 0:\n        continue\n    res_df = pd.concat([res_df, res_read[['text', 'dataset']]], axis=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"records = res_df.to_dict(orient='records')\n\n# Flatten: each relation becomes one row, and we pull in the dataset fields as \"meta\"\nflat = pd.json_normalize(\n    records,\n    # record_path='relations',\n    meta=[\n      ['dataset','start'],\n      ['dataset','end'],\n      ['dataset','text'],\n      ['dataset','label'],\n      ['dataset','score']\n    ],\n)\n\nflat = flat.rename(columns={\n    'dataset.start': 'start',\n    'dataset.end':   'end',\n    'dataset.text':  'dataset',\n    'dataset.label': 'ds_label',\n    'dataset.score': 'ds_score',\n    'score':         'rel_score'   # this was the relation’s confidence\n})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm.auto import tqdm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"res_df = pd.DataFrame()\nfor fname in tqdm(fnames, desc=\"processing dfs\"):\n    res_read = pd.read_json(fname)\n    if res_read.shape[0] == 0:\n        continue\n    res_df = pd.concat([res_df, res_read[['dataset', 'relations']]], axis=0)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"res_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\nrecords = res_df.to_dict(orient='records')\n\n# Flatten\nflat = pd.json_normalize(\n    records,\n    record_path='relations',\n    meta=[\n      ['dataset','start'],\n      ['dataset','end'],\n      ['dataset','text'],\n      ['dataset','label'],\n      ['dataset','score']\n    ],\n)\n\n# Rename columns for clarity\nflat = flat.rename(columns={\n    'dataset.start': 'start',\n    'dataset.end':   'end',\n    'dataset.text':  'ds_text',\n    'dataset.label': 'ds_label',\n    'dataset.score': 'ds_score',\n    'score':         'rel_score'   \n})\nflat.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Deduplicate to one best relation per (dataset, relation) pair\nflat = (\n    flat\n      .sort_values('rel_score', ascending=False)\n      .drop_duplicates(subset=['ds_text','relation'])\n      .reset_index(drop=True)\n)\n\n# Pivot\nmeta = (\n    flat\n      .pivot(index=['ds_text','ds_label','ds_score','start','end'],\n             columns='relation',\n             values='target')\n      .reset_index()\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"meta.loc[meta['publisher'].isna()]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"meta","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from rapidfuzz import process, fuzz\nimport re\n\ndef make_canon_map(strings, score_cutoff=85):\n    clusters = {}\n    for s in strings:\n        match = process.extractOne(s, clusters.keys(),\n                                   scorer=fuzz.token_sort_ratio,\n                                   score_cutoff=score_cutoff)\n        if match:\n            clusters[match[0]].append(s)\n        else:\n            clusters[s] = [s]\n    return {v: k for k, vs in clusters.items() for v in vs}, clusters\n\n# standardize_years\ndef standardize_year_range(s):\n    if pd.isna(s):\n        return None\n    yrs = [int(m.group(1))\n           for m in re.finditer(r'\\b((?:19|20)\\d{2})\\b', s)]\n    if len(yrs) == 1:\n        return f\"{yrs[0]}\"\n    if len(yrs) >= 2:\n        y1, y2 = sorted(yrs[:2])\n        return f\"{y1}-{y2}\"\n    try:\n        y = parse(s, fuzzy=True).year\n        return f\"{y}\"\n    except:\n        return None\n\nmeta['year_range'] = meta['years'].apply(standardize_year_range)\n\n# build maps from the pivoted columns\npub_map,  pub_clusters  = make_canon_map(meta['publisher'].dropna().unique(),  score_cutoff=80)\ngeo_map,  geo_clusters  = make_canon_map(meta['geography'].dropna().unique(),  score_cutoff=80)\nabbr_map, abbr_clusters = make_canon_map(meta['abbreviation'].dropna().unique(), score_cutoff=80)\nyear_map,      year_clusters  = make_canon_map(meta['year_range'].dropna().unique(),  score_cutoff=100)\nds_map, ds_clusters = make_canon_map(meta['ds_text'].dropna().unique(), score_cutoff=80)\ndesc_map, desc_clusters = make_canon_map(\n    meta['description'].dropna().unique().tolist(),\n    score_cutoff=80\n)\n\nmeta['description_canon'] = meta['description'].map(desc_map)\nmeta['publisher_canon']    = meta['publisher'].map(pub_map)\nmeta['geography_canon']    = meta['geography'].map(geo_map)\nmeta['abbreviation_canon'] = meta['abbreviation'].map(abbr_map)\nmeta['year_canon'] = meta['year_range'].map(year_map)\nmeta['ds_canon'] = meta['ds_text'].map(ds_map)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reagg = (\n    meta\n      .sort_values('ds_score', ascending=False)   # keep best‐scoring rows if duplicates\n      .drop_duplicates(\n         subset=['ds_canon','publisher_canon','geography_canon','abbreviation_canon', 'year_canon','description_canon']\n      )\n      .reset_index(drop=True)\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reagg","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import plotly.express as px\n\ncounts_all = (\n    reagg\n    .groupby([\n        'publisher_canon',\n        'geography_canon',\n        'year_canon',\n        'ds_canon',\n        'description_canon',\n    ])\n    .agg(\n        size       = ('ds_score','count'),\n        avg_score  = ('ds_score','mean')\n    )\n    .reset_index()\n)\n\nfig = px.treemap(\n  counts_all,\n    path=[\n        'publisher_canon',\n        'geography_canon',\n        'year_canon',\n        'ds_canon',\n        'description_canon',\n    ],\n  values='size', color='avg_score',\ncolor_continuous_scale='Viridis',\ntitle='Drill‐down: Publisher → Geography → Year → Dataset → Description'\n)\nfig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Top Datasets by Geography","metadata":{}},{"cell_type":"code","source":"import textwrap\nimport plotly.graph_objects as go\n\ndef plot_top_dropdown(\n    df,\n    group_col: str,\n    item_col: str,\n    top_n: int = 10,\n    menu_x: float = 0.99,\n    menu_y: float = 0.99,\n    margin: dict = None,\n    wrap_width: int = 20,\n):\n    \"\"\"\n    Creates an interactive horizontal‐bar chart with a dropdown to switch groups.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Must contain columns `group_col` and `item_col`.\n    group_col : str\n        Column to group by (e.g. 'year_canon').\n    item_col : str\n        Column of items to count (e.g. 'ds_canon').\n    top_n : int\n        Number of top items per group.\n    menu_x, menu_y : float\n        Position of the dropdown (0−1 coordinates).\n    margin : dict or None\n        Plot margins; defaults to {'l':300,'r':50,'t':50,'b':50}.\n    wrap_width : int\n        Max chars before wrapping dropdown labels.\n    \"\"\"\n    if margin is None:\n        margin = dict(l=300, r=50, t=50, b=50)\n\n    def wrap_label(label):\n        return \"<br>\".join(textwrap.wrap(str(label), wrap_width))\n\n    # 1) count occurrences\n    counts = (\n        df\n        .groupby([group_col, item_col])\n        .size()\n        .reset_index(name=\"count\")\n    )\n\n    # 2) sort groups by total\n    group_totals = counts.groupby(group_col)[\"count\"].sum()\n    groups_sorted = group_totals.sort_values(ascending=False).index.tolist()\n\n    # 3) create one trace per group\n    traces = []\n    for grp in groups_sorted:\n        subset = (\n            counts[counts[group_col] == grp]\n            .nlargest(top_n, \"count\")\n            .sort_values(\"count\", ascending=True)\n        )\n        traces.append(go.Bar(\n            x=subset[\"count\"],\n            y=subset[item_col],\n            orientation=\"h\",\n            name=str(grp),\n            visible=(grp == groups_sorted[0])\n        ))\n\n    # 4) build dropdown buttons\n    buttons = []\n    for i, grp in enumerate(groups_sorted):\n        visible = [j == i for j in range(len(groups_sorted))]\n        buttons.append(dict(\n            label=wrap_label(grp),\n            method=\"update\",\n            args=[\n                {\"visible\": visible},\n                {\n                    \"title\": f\"Top Datasets per {group_col.replace('_canon','')}\",\n                    \"margin\": margin\n                }\n            ]\n        ))\n\n    # 5) assemble figure\n    fig = go.Figure(data=traces)\n    fig.update_layout(\n        updatemenus=[dict(\n            active=0,\n            buttons=buttons,\n            x=menu_x, xanchor=\"right\",\n            y=menu_y, yanchor=\"top\",\n            direction=\"down\",\n        )],\n        title=f\"Top Datasets per {group_col.replace('_canon','')}\",\n        xaxis_title=\"Count\",\n        yaxis_title=item_col.replace(\"_\", \" \").title(),\n        showlegend=False,\n        margin=margin\n    )\n    fig.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Top 10 datasets by year\nplot_top_dropdown(reagg, \"geography_canon\", \"ds_canon\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Top Datasets by Publisher","metadata":{}},{"cell_type":"code","source":"plot_top_dropdown(reagg, \"publisher_canon\", \"ds_canon\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Top Datasets by Year","metadata":{}},{"cell_type":"code","source":"plot_top_dropdown(reagg, \"year_canon\", \"ds_canon\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}